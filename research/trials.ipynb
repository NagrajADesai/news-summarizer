{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.news_scraper import main\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for news about Tesla...\n",
      "Scraping https://news.google.com/search?q=Tesla&hl=en-US&gl=US&ceid=US:en\n",
      "Scraping https://www.reuters.com/search/news?blob=Tesla\n",
      "Scraping https://www.businesstoday.in/search.jsp?searchword=Tesla&searchtype=text&searchphrase=exact\n",
      "Scraping https://economictimes.indiatimes.com/searchresult.cms?query=Tesla\n",
      "Scraping https://www.livemint.com/searchlisting/Tesla\n",
      "Found 0 unique articles about Tesla\n",
      "Results saved to news_articles/tesla_news_20250319.json\n",
      "\n",
      "Sample titles:\n",
      "Got 0 articles\n"
     ]
    }
   ],
   "source": [
    "company_name = \"Tesla\"\n",
    "result = main(company_name)\n",
    "print(f\"Got {result['total_articles']} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found\n"
     ]
    }
   ],
   "source": [
    "# If you're getting results but DataFrame is empty, check the articles\n",
    "if not result['articles']:\n",
    "    print(\"No articles found\")\n",
    "else:\n",
    "    df = pd.DataFrame(result['articles'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "\n",
    "class NewsArticleScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Referer': 'https://www.google.com/'\n",
    "        }\n",
    "        # List of news sources with updated selectors\n",
    "        self.news_sources = [\n",
    "            {\"url\": \"https://news.google.com/search?q={}&hl=en-US&gl=US&ceid=US:en\", \"type\": \"google_news\"},\n",
    "            {\"url\": \"https://economictimes.indiatimes.com/searchresult.cms?query={}\", \"type\": \"economic_times\"},\n",
    "            {\"url\": \"https://finance.yahoo.com/quote/{}/news\", \"type\": \"yahoo_finance\"},\n",
    "            {\"url\": \"https://www.ndtv.com/search?searchtext={}\", \"type\": \"ndtv\"}\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # Remove extra whitespace and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def extract_date(self, date_str):\n",
    "        try:\n",
    "            # Handle common date formats\n",
    "            date_str = self.clean_text(date_str)\n",
    "            if not date_str:\n",
    "                return None\n",
    "                \n",
    "            # Try to parse various date formats\n",
    "            date_formats = [\n",
    "                \"%Y-%m-%d\",\n",
    "                \"%d %b %Y\", \n",
    "                \"%B %d, %Y\",\n",
    "                \"%d/%m/%Y\",\n",
    "                \"%m/%d/%Y\"\n",
    "            ]\n",
    "            \n",
    "            for fmt in date_formats:\n",
    "                try:\n",
    "                    return datetime.strptime(date_str, fmt).strftime(\"%Y-%m-%d\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # If no format matches, return the original string\n",
    "            return date_str\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def scrape_google_news(self, soup):\n",
    "        articles = []\n",
    "        print(\"Attempting to scrape Google News...\")\n",
    "        \n",
    "        # Save the HTML for debugging\n",
    "        with open(\"google_news_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(soup))\n",
    "        \n",
    "        # Try different selectors\n",
    "        selectors = [\n",
    "            \"article\",\n",
    "            \".DBQmFf\",\n",
    "            \".NiLAwe\",\n",
    "            \".lBwEZb\",\n",
    "            \".h4VLc\"\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            article_elements = soup.select(selector)\n",
    "            print(f\"Found {len(article_elements)} elements with selector '{selector}'\")\n",
    "            \n",
    "            if len(article_elements) > 0:\n",
    "                break\n",
    "        \n",
    "        if len(article_elements) == 0:\n",
    "            print(\"No articles found in Google News. Trying alternative approach.\")\n",
    "            # Fall back to a more general approach\n",
    "            article_elements = soup.find_all(\"div\", class_=lambda x: x and (\"NiLAwe\" in x or \"lBwEZb\" in x))\n",
    "            print(f\"Found {len(article_elements)} elements with alternative approach\")\n",
    "        \n",
    "        for i, article in enumerate(article_elements[:20]):\n",
    "            try:\n",
    "                # Extract title - try multiple selectors\n",
    "                title_element = article.select_one(\"h3 a, h4 a, a[data-n-tid]\")\n",
    "                if not title_element:\n",
    "                    title_element = article.find(\"a\", class_=lambda x: x and \"DY5T1d\" in x)\n",
    "                \n",
    "                if not title_element:\n",
    "                    continue\n",
    "                \n",
    "                title = self.clean_text(title_element.text)\n",
    "                if not title:\n",
    "                    continue\n",
    "                \n",
    "                # Extract summary\n",
    "                summary_element = article.select_one(\"h4, .Rai5ob, .xBbh9\")\n",
    "                summary = self.clean_text(summary_element.text) if summary_element else \"\"\n",
    "                \n",
    "                # Extract source\n",
    "                source_element = article.select_one(\"div[data-n-tid] a, .wEwyrc, .SVJrMe\")\n",
    "                source = self.clean_text(source_element.text) if source_element else \"\"\n",
    "                \n",
    "                # Extract date\n",
    "                time_element = article.select_one(\"time, .WW6dff\")\n",
    "                published_date = time_element.get(\"datetime\") if time_element and time_element.has_attr(\"datetime\") else None\n",
    "                if not published_date and time_element:\n",
    "                    published_date = self.clean_text(time_element.text)\n",
    "                \n",
    "                # Get link\n",
    "                link = \"\"\n",
    "                if title_element.has_attr(\"href\"):\n",
    "                    link_element = title_element[\"href\"]\n",
    "                    if link_element.startswith(\"/\"):\n",
    "                        link = \"https://news.google.com\" + link_element\n",
    "                    else:\n",
    "                        link = link_element\n",
    "                \n",
    "                if title:\n",
    "                    articles.append({\n",
    "                        \"title\": title,\n",
    "                        \"summary\": summary,\n",
    "                        \"source\": source,\n",
    "                        \"published_date\": published_date,\n",
    "                        \"url\": link,\n",
    "                        \"scraper_source\": \"google_news\"\n",
    "                    })\n",
    "                \n",
    "                print(f\"Found article {i+1}: {title[:50]}...\")\n",
    "                \n",
    "                if len(articles) >= 10:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing Google News article {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully scraped {len(articles)} articles from Google News\")\n",
    "        return articles\n",
    "    \n",
    "    # def scrape_economic_times(self, soup):\n",
    "    #     articles = []\n",
    "    #     print(\"Attempting to scrape Economic Times...\")\n",
    "        \n",
    "    #     article_elements = soup.select(\".article, .eachStory\")\n",
    "    #     print(f\"Found {len(article_elements)} elements with selector '.article, .eachStory'\")\n",
    "        \n",
    "    #     for i, article in enumerate(article_elements[:15]):\n",
    "    #         try:\n",
    "    #             # Extract title\n",
    "    #             title_element = article.select_one(\"h3 a, h2 a, .title a\")\n",
    "    #             if not title_element:\n",
    "    #                 continue\n",
    "                    \n",
    "    #             title = self.clean_text(title_element.text)\n",
    "                \n",
    "    #             # Extract summary\n",
    "    #             summary_element = article.select_one(\".abstract, .desc\")\n",
    "    #             summary = self.clean_text(summary_element.text) if summary_element else \"\"\n",
    "                \n",
    "    #             # Extract date\n",
    "    #             date_element = article.select_one(\".date-format, .date\")\n",
    "    #             published_date = self.extract_date(date_element.text) if date_element else None\n",
    "                \n",
    "    #             # Extract link\n",
    "    #             link = title_element.get(\"href\") if title_element else None\n",
    "    #             if link and not link.startswith(\"http\"):\n",
    "    #                 link = \"https://economictimes.indiatimes.com\" + link\n",
    "                \n",
    "    #             articles.append({\n",
    "    #                 \"title\": title,\n",
    "    #                 \"summary\": summary,\n",
    "    #                 \"source\": \"Economic Times\",\n",
    "    #                 \"published_date\": published_date,\n",
    "    #                 \"url\": link,\n",
    "    #                 \"scraper_source\": \"economic_times\"\n",
    "    #             })\n",
    "                \n",
    "    #             print(f\"Found article {i+1}: {title[:50]}...\")\n",
    "                \n",
    "    #             if len(articles) >= 10:\n",
    "    #                 break\n",
    "                    \n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error parsing Economic Times article {i+1}: {e}\")\n",
    "    #             continue\n",
    "        \n",
    "    #     print(f\"Successfully scraped {len(articles)} articles from Economic Times\")\n",
    "    #     return articles\n",
    "    \n",
    "    # def scrape_yahoo_finance(self, soup):\n",
    "    #     articles = []\n",
    "    #     print(\"Attempting to scrape Yahoo Finance...\")\n",
    "        \n",
    "    #     article_elements = soup.select(\"li.js-stream-content\")\n",
    "    #     print(f\"Found {len(article_elements)} elements with selector 'li.js-stream-content'\")\n",
    "        \n",
    "    #     for i, article in enumerate(article_elements[:15]):\n",
    "    #         try:\n",
    "    #             # Extract title\n",
    "    #             title_element = article.select_one(\"h3\")\n",
    "    #             if not title_element:\n",
    "    #                 continue\n",
    "                    \n",
    "    #             title = self.clean_text(title_element.text)\n",
    "                \n",
    "    #             # Extract summary\n",
    "    #             summary_element = article.select_one(\"p\")\n",
    "    #             summary = self.clean_text(summary_element.text) if summary_element else \"\"\n",
    "                \n",
    "    #             # Extract source and date\n",
    "    #             source_element = article.select_one(\".C(#959595)\")\n",
    "    #             source_text = self.clean_text(source_element.text) if source_element else \"\"\n",
    "    #             source = source_text.split('·')[0].strip() if '·' in source_text else source_text\n",
    "                \n",
    "    #             published_date = None\n",
    "    #             if '·' in source_text:\n",
    "    #                 date_part = source_text.split('·')[1].strip()\n",
    "    #                 published_date = self.extract_date(date_part)\n",
    "                \n",
    "    #             # Extract link\n",
    "    #             link_element = article.select_one(\"a\")\n",
    "    #             link = link_element.get(\"href\") if link_element else None\n",
    "    #             if link and not link.startswith(\"http\"):\n",
    "    #                 link = \"https://finance.yahoo.com\" + link\n",
    "                \n",
    "    #             articles.append({\n",
    "    #                 \"title\": title,\n",
    "    #                 \"summary\": summary,\n",
    "    #                 \"source\": source or \"Yahoo Finance\",\n",
    "    #                 \"published_date\": published_date,\n",
    "    #                 \"url\": link,\n",
    "    #                 \"scraper_source\": \"yahoo_finance\"\n",
    "    #             })\n",
    "                \n",
    "    #             print(f\"Found article {i+1}: {title[:50]}...\")\n",
    "                \n",
    "    #             if len(articles) >= 10:\n",
    "    #                 break\n",
    "                    \n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error parsing Yahoo Finance article {i+1}: {e}\")\n",
    "    #             continue\n",
    "        \n",
    "    #     print(f\"Successfully scraped {len(articles)} articles from Yahoo Finance\")\n",
    "    #     return articles\n",
    "    \n",
    "    # def scrape_ndtv(self, soup):\n",
    "    #     articles = []\n",
    "    #     print(\"Attempting to scrape NDTV...\")\n",
    "        \n",
    "    #     article_elements = soup.select(\".news_Itm\")\n",
    "    #     print(f\"Found {len(article_elements)} elements with selector '.news_Itm'\")\n",
    "        \n",
    "    #     for i, article in enumerate(article_elements[:15]):\n",
    "    #         try:\n",
    "    #             # Extract title\n",
    "    #             title_element = article.select_one(\".newsHdng\")\n",
    "    #             if not title_element:\n",
    "    #                 continue\n",
    "                    \n",
    "    #             title = self.clean_text(title_element.text)\n",
    "                \n",
    "    #             # Extract summary\n",
    "    #             summary_element = article.select_one(\".newsCont\")\n",
    "    #             summary = self.clean_text(summary_element.text) if summary_element else \"\"\n",
    "                \n",
    "    #             # Extract date\n",
    "    #             date_element = article.select_one(\".posted-on\")\n",
    "    #             published_date = self.extract_date(date_element.text) if date_element else None\n",
    "                \n",
    "    #             # Extract link\n",
    "    #             link_element = title_element.find(\"a\") if title_element else None\n",
    "    #             link = link_element.get(\"href\") if link_element else None\n",
    "                \n",
    "    #             articles.append({\n",
    "    #                 \"title\": title,\n",
    "    #                 \"summary\": summary,\n",
    "    #                 \"source\": \"NDTV\",\n",
    "    #                 \"published_date\": published_date,\n",
    "    #                 \"url\": link,\n",
    "    #                 \"scraper_source\": \"ndtv\"\n",
    "    #             })\n",
    "                \n",
    "    #             print(f\"Found article {i+1}: {title[:50]}...\")\n",
    "                \n",
    "    #             if len(articles) >= 10:\n",
    "    #                 break\n",
    "                    \n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error parsing NDTV article {i+1}: {e}\")\n",
    "    #             continue\n",
    "        \n",
    "    #     print(f\"Successfully scraped {len(articles)} articles from NDTV\")\n",
    "    #     return articles\n",
    "    \n",
    "    def get_articles_for_company(self, company_name, num_articles=10):\n",
    "        all_articles = []\n",
    "        \n",
    "        # Try each news source until we have enough articles\n",
    "        for source in self.news_sources:\n",
    "            if len(all_articles) >= num_articles:\n",
    "                break\n",
    "            \n",
    "            # Format URL with company name\n",
    "            url = source[\"url\"].format(quote(company_name))\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nScraping {url}\")\n",
    "                response = requests.get(url, headers=self.headers, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # Choose appropriate scraping method based on source type\n",
    "                    if source[\"type\"] == \"google_news\":\n",
    "                        articles = self.scrape_google_news(soup)\n",
    "                    # elif source[\"type\"] == \"economic_times\":\n",
    "                    #     articles = self.scrape_economic_times(soup)\n",
    "                    # elif source[\"type\"] == \"yahoo_finance\":\n",
    "                    #     articles = self.scrape_yahoo_finance(soup)\n",
    "                    # elif source[\"type\"] == \"ndtv\":\n",
    "                    #     articles = self.scrape_ndtv(soup)\n",
    "                    else:\n",
    "                        articles = []\n",
    "                    \n",
    "                    # Add new articles to our collection\n",
    "                    for article in articles:\n",
    "                        # Check for duplicates by title similarity\n",
    "                        if not any(self.is_similar_title(article[\"title\"], existing[\"title\"]) for existing in all_articles):\n",
    "                            all_articles.append(article)\n",
    "                    \n",
    "                    print(f\"Total unique articles so far: {len(all_articles)}\")\n",
    "                    \n",
    "                    # Break if we have enough articles\n",
    "                    if len(all_articles) >= num_articles:\n",
    "                        break\n",
    "                        \n",
    "                    # Be nice to the servers\n",
    "                    time.sleep(random.uniform(1, 3))\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Failed to load {source['type']}: Status code {response.status_code}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {source['type']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Return only the number of articles requested\n",
    "        return all_articles[:num_articles]\n",
    "    \n",
    "    def is_similar_title(self, title1, title2):\n",
    "        \"\"\"Check if two titles are similar to avoid duplicates\"\"\"\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        t1 = re.sub(r'[^\\w\\s]', '', title1.lower())\n",
    "        t2 = re.sub(r'[^\\w\\s]', '', title2.lower())\n",
    "        \n",
    "        # If one title is contained in the other, consider them similar\n",
    "        if t1 in t2 or t2 in t1:\n",
    "            return True\n",
    "            \n",
    "        # Calculate word overlap\n",
    "        words1 = set(t1.split())\n",
    "        words2 = set(t2.split())\n",
    "        \n",
    "        # If they share more than 70% of words, consider them similar\n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return False\n",
    "            \n",
    "        overlap = len(words1.intersection(words2)) / min(len(words1), len(words2))\n",
    "        return overlap > 0.7\n",
    "\n",
    "def main(company_name=None):\n",
    "    scraper = NewsArticleScraper()\n",
    "    \n",
    "    # Get company name from user or use the provided one\n",
    "    if company_name is None:\n",
    "        company_name = input(\"Enter company name to search for: \")\n",
    "    \n",
    "    print(f\"Searching for news about {company_name}...\")\n",
    "    articles = scraper.get_articles_for_company(company_name)\n",
    "    \n",
    "    # Output results as JSON\n",
    "    output = {\n",
    "        \"company\": company_name,\n",
    "        \"articles\": articles,\n",
    "        \"total_articles\": len(articles),\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    print(f\"Found {len(articles)} unique articles about {company_name}\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(\"news_articles\"):\n",
    "        os.makedirs(\"news_articles\")\n",
    "    \n",
    "    # Save to file\n",
    "    filename = f\"news_articles/{company_name.replace(' ', '_').lower()}_news_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    # Print sample titles\n",
    "    print(\"\\nSample titles:\")\n",
    "    for i, article in enumerate(articles[:5], 1):\n",
    "        print(f\"{i}. {article['title']}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for news about Tesla...\n",
      "\n",
      "Scraping https://news.google.com/search?q=Tesla&hl=en-US&gl=US&ceid=US:en\n",
      "Attempting to scrape Google News...\n",
      "Found 93 elements with selector 'article'\n",
      "Found article 1: Pam Bondi Calls Tesla Vandalism ‘Domestic Terroris...\n",
      "Found article 2: FBI Investigating Tesla Vandalism as GOP Decries T...\n",
      "Found article 3: Attorney general calls Tesla arson attacks 'nothin...\n",
      "Found article 4: Violent attacks on Tesla dealerships spike as Musk...\n",
      "Found article 5: Tesla investor calls for Elon Musk to step down as...\n",
      "Found article 6: Tesla Stock Slides Another 5% As More Firms Warn O...\n",
      "Found article 7: 4 reasons why Tesla's 53% stock crash is accelerat...\n",
      "Found article 8: A CEO says his solar-panel company bought a new Te...\n",
      "Found article 9: Tesla’s Gamble on MAGA Customers Won’t Work...\n",
      "Found article 10: Teslas in Las Vegas set on fire and shot with guns...\n",
      "Successfully scraped 10 articles from Google News\n",
      "Total unique articles so far: 10\n",
      "Found 10 unique articles about Tesla\n",
      "Results saved to news_articles/tesla_news_20250319.json\n",
      "\n",
      "Sample titles:\n",
      "1. Pam Bondi Calls Tesla Vandalism ‘Domestic Terrorism,’ Promising Consequences\n",
      "2. FBI Investigating Tesla Vandalism as GOP Decries Terrorism\n",
      "3. Attorney general calls Tesla arson attacks 'nothing short of domestic terrorism'\n",
      "4. Violent attacks on Tesla dealerships spike as Musk takes prominent role in Trump White House\n",
      "5. Tesla investor calls for Elon Musk to step down as boss\n",
      "Successfully retrieved 10 articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>published_date</th>\n",
       "      <th>url</th>\n",
       "      <th>scraper_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pam Bondi Calls Tesla Vandalism ‘Domestic Terr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-19T03:40:56Z</td>\n",
       "      <td>./read/CBMijwFBVV95cUxPTEJiSTVsMUpOcTJ0U20xMVA...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FBI Investigating Tesla Vandalism as GOP Decri...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T23:01:52Z</td>\n",
       "      <td>./read/CBMirgFBVV95cUxOMGpEOWQ0VHQ0VU10SkJQdnJ...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attorney general calls Tesla arson attacks 'no...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T22:39:59Z</td>\n",
       "      <td>./read/CBMinAFBVV95cUxNUkdaSW1MUVhjZW9kTE5UMml...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Violent attacks on Tesla dealerships spike as ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-19T04:03:00Z</td>\n",
       "      <td>./read/CBMiqAFBVV95cUxOZlVRN2tVTFFNM3YyZWt5akJ...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tesla investor calls for Elon Musk to step dow...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T17:47:43Z</td>\n",
       "      <td>./read/CBMilgFBVV95cUxNNEQtVUoxc2hWc1o1cFZBLW1...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tesla Stock Slides Another 5% As More Firms Wa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T20:06:00Z</td>\n",
       "      <td>./read/CBMiygFBVV95cUxQYzVsc1h2THdVVm4zbTZWeU5...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4 reasons why Tesla's 53% stock crash is accel...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T23:36:00Z</td>\n",
       "      <td>./read/CBMinwFBVV95cUxOR3p6NlBKc3h5S3BxN3RuY0t...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A CEO says his solar-panel company bought a ne...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-17T17:34:00Z</td>\n",
       "      <td>./read/CBMiiwFBVV95cUxNMWVZMWlEenNpSUVRdm5Tenl...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tesla’s Gamble on MAGA Customers Won’t Work</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T10:00:13Z</td>\n",
       "      <td>./read/CBMiogFBVV95cUxPMWxzZ0l3SzJ4NzVQeDdLTFF...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Teslas in Las Vegas set on fire and shot with ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-18T21:46:00Z</td>\n",
       "      <td>./read/CBMiZ0FVX3lxTE9LZnVTMFVJRWt4WU9RRC1KWTJ...</td>\n",
       "      <td>google_news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title summary source  \\\n",
       "0  Pam Bondi Calls Tesla Vandalism ‘Domestic Terr...                  \n",
       "1  FBI Investigating Tesla Vandalism as GOP Decri...                  \n",
       "2  Attorney general calls Tesla arson attacks 'no...                  \n",
       "3  Violent attacks on Tesla dealerships spike as ...                  \n",
       "4  Tesla investor calls for Elon Musk to step dow...                  \n",
       "5  Tesla Stock Slides Another 5% As More Firms Wa...                  \n",
       "6  4 reasons why Tesla's 53% stock crash is accel...                  \n",
       "7  A CEO says his solar-panel company bought a ne...                  \n",
       "8        Tesla’s Gamble on MAGA Customers Won’t Work                  \n",
       "9  Teslas in Las Vegas set on fire and shot with ...                  \n",
       "\n",
       "         published_date                                                url  \\\n",
       "0  2025-03-19T03:40:56Z  ./read/CBMijwFBVV95cUxPTEJiSTVsMUpOcTJ0U20xMVA...   \n",
       "1  2025-03-18T23:01:52Z  ./read/CBMirgFBVV95cUxOMGpEOWQ0VHQ0VU10SkJQdnJ...   \n",
       "2  2025-03-18T22:39:59Z  ./read/CBMinAFBVV95cUxNUkdaSW1MUVhjZW9kTE5UMml...   \n",
       "3  2025-03-19T04:03:00Z  ./read/CBMiqAFBVV95cUxOZlVRN2tVTFFNM3YyZWt5akJ...   \n",
       "4  2025-03-18T17:47:43Z  ./read/CBMilgFBVV95cUxNNEQtVUoxc2hWc1o1cFZBLW1...   \n",
       "5  2025-03-18T20:06:00Z  ./read/CBMiygFBVV95cUxQYzVsc1h2THdVVm4zbTZWeU5...   \n",
       "6  2025-03-18T23:36:00Z  ./read/CBMinwFBVV95cUxOR3p6NlBKc3h5S3BxN3RuY0t...   \n",
       "7  2025-03-17T17:34:00Z  ./read/CBMiiwFBVV95cUxNMWVZMWlEenNpSUVRdm5Tenl...   \n",
       "8  2025-03-18T10:00:13Z  ./read/CBMiogFBVV95cUxPMWxzZ0l3SzJ4NzVQeDdLTFF...   \n",
       "9  2025-03-18T21:46:00Z  ./read/CBMiZ0FVX3lxTE9LZnVTMFVJRWt4WU9RRC1KWTJ...   \n",
       "\n",
       "  scraper_source  \n",
       "0    google_news  \n",
       "1    google_news  \n",
       "2    google_news  \n",
       "3    google_news  \n",
       "4    google_news  \n",
       "5    google_news  \n",
       "6    google_news  \n",
       "7    google_news  \n",
       "8    google_news  \n",
       "9    google_news  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure you have the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "\n",
    "# Copy and paste the entire NewsArticleScraper class and main function here\n",
    "\n",
    "# Run the scraper\n",
    "company_name = \"Tesla\"\n",
    "result = main(company_name)\n",
    "\n",
    "# Check if we got any results\n",
    "if result and result['articles']:\n",
    "    print(f\"Successfully retrieved {len(result['articles'])} articles\")\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(result['articles'])\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No articles found or error occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
